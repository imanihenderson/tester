{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31164676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying KeyBERT for keyword extraction\n",
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fa696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocKeywordIndex:\n",
    "    \"\"\"\n",
    "    Build once per epoch (or cache to disk).\n",
    "    Produces doc-level keyphrases and a doc embedding for quick re-scoring.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 doc_texts: Dict[str, str],  # doc_id -> full text\n",
    "                 keybert_model: str = \"all-MiniLM-L6-v2\",\n",
    "                 top_n: int = 40,\n",
    "                 ngram_range=(1,3),\n",
    "                 use_mmr: bool = True,\n",
    "                 diversity: float = 0.6):\n",
    "        self.kb = KeyBERT(model=keybert_model)\n",
    "        self.embedder = SentenceTransformer(keybert_model)\n",
    "        self.doc_keywords: Dict[str, List[str]] = {}\n",
    "        self.doc_embs: Dict[str, np.ndarray] = {}\n",
    "        self.ngram_range = ngram_range\n",
    "\n",
    "        for doc_id, text in doc_texts.items():\n",
    "            # KeyBERT over the full doc\n",
    "            kws = self.kb.extract_keywords(\n",
    "                text,\n",
    "                keyphrase_ngram_range=ngram_range,\n",
    "                stop_words=\"english\",\n",
    "                use_mmr=use_mmr,\n",
    "                diversity=diversity,\n",
    "                top_n=top_n\n",
    "            )\n",
    "            self.doc_keywords[doc_id] = [k for (k, score) in kws]\n",
    "            # store doc embedding for quick cosine scoring later\n",
    "            self.doc_embs[doc_id] = self.embedder.encode([text], normalize_embeddings=True)[0]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
