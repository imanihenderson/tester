{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feabb3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_clip.py\n",
    "import os\n",
    "import math\n",
    "import csv\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from keybert import KeyBERT\n",
    "# ^ pip install keybert \n",
    "from sentence_transformers import SentenceTransformer\n",
    "# ^ pip install sentence-transformers\n",
    "from typing import Dict, Tuple, Optional, Iterable, Any, List as _List\n",
    "# avoid name clashes with List \n",
    "\n",
    "import open_clip\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    # extending for KeyBERT usage\n",
    "    use_keybert: bool = True\n",
    "    keybert_model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"  # from sentence-transformers\n",
    "    keybert_top_n: int = 40 # num keywords to extract per caption; will be trimmed to tokens \n",
    "    keybert_ngram_range: Tuple[int,int] = (1, 3) # 1-3 word phrases\n",
    "    keybert_use_mmr: bool = True #  maximum marginal relevance to avoid redundancy but maintain relevance \n",
    "    keybert_diversity: float = 0.6 # redundancy penalty for mmr \n",
    "    max_text_tokens: int = 75 # max tokens for text encoder (CLIP models typically 77)\n",
    "    cache_keywords: bool = True # cache keywords to avoid recomputation\n",
    "    # original config options\n",
    "    model_name: str = \"ViT-B-32\"\n",
    "    pretrained: str = \"laion400m_e32\"   # or laion400m_e31\n",
    "    train_csv: str = \"data/train.csv\"\n",
    "    batch_size: int = 64\n",
    "    num_workers: int = 4\n",
    "    max_epochs: int = 5\n",
    "    lr: float = 5e-5\n",
    "    weight_decay: float = 0.02\n",
    "    grad_clip: float = 1.0\n",
    "    amp: bool = True\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    out_dir: str = \"checkpoints\"\n",
    "    finetune_mode: str = \"proj_only\"  # one of: \"full\", \"proj_only\", \"text_only\", \"vision_only\"\n",
    "    seed: int = 42\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "torch.manual_seed(cfg.seed)\n",
    "os.makedirs(cfg.out_dir, exist_ok=True)\n",
    "\n",
    "#token aware packing of keywords\n",
    "def pack_for_tokenizer(phrases: _List[str], tokenizer, max_tokens: int = 75, sep: str = \"; \") -> str:\n",
    "    \"\"\"\n",
    "    Greedily adds phrases by concatening phrases with ';' until tokenized length hits budget\"\"\"\n",
    "\n",
    "    if not phrases: \n",
    "        return \"\"\n",
    "    packed = []\n",
    "    for p in phrases:\n",
    "        candidate = sep.join(packed + [p])\n",
    "        tok_len = int(len(tokenizer([candidate])[0]))\n",
    "        if tok_len <= max_tokens:\n",
    "            packed.append(p)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return sep.join(packed) if packed else \"\"\n",
    "\n",
    "class KeywordCompressor:\n",
    "    \"\"\"\n",
    "    Extracts KeyBERT keyphrases for a text, then trims to\n",
    "    CLIP token budget w/ pack for tokenizer\n",
    "\n",
    "    keeps sentencetransformer model for future re-ranking \n",
    "\n",
    "    optional cache to speed repeated captions \n",
    "\n",
    "    \"\"\"\n",
    "    def _init_(self, model_name: str, top_n: int, ngram_range: Tuple[int, int],\n",
    "               use_mmr: bool, diversity: float, tokenizer, max_text_tokens: int=75,\n",
    "               enable_cache: bool = True):\n",
    "        self.kb = KeyBERT(model=model_name)\n",
    "        self.embedder = SentenceTransformer(model_name)\n",
    "        self.top_n = top_n\n",
    "        self.ngram_range = ngram_range\n",
    "        self.use_mmr = use_mmr\n",
    "        self.diversity = diversity\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_text_tokens = max_text_tokens\n",
    "        self.cache: Dict[str, str] = {} if enable_cache else None\n",
    "\n",
    "    def _call_(self, text: str) -> str:\n",
    "        if self.cache is not None and text in self.cache:\n",
    "            return self.cache[text]\n",
    "        \n",
    "        # build semantic pool for KeyBERT\n",
    "        kws = self.kb.extract_keywords(\n",
    "            text,\n",
    "            keyphrase_ngram_range=self.ngram_range,\n",
    "            stop_words=\"english\",\n",
    "            use_mmr=self.use_mmr,\n",
    "            diversity=self.diversity,\n",
    "            top_n=self.top_n,\n",
    "        )\n",
    "        phrases = [k for (k, _score) in kws]\n",
    "\n",
    "        # trim with respect to tokenizer budget\n",
    "        compressed = pack_for_tokenizer(\n",
    "            phrases, tokenizer=self.tokenizer, max_tokens=self.max_text_tokens, sep=\"; \"\n",
    "        )\n",
    "\n",
    "        # cache and return compressed text\n",
    "\n",
    "        # in the event packing yields empty, let CLIP handle truncation\n",
    "        if not compressed: \n",
    "            compressed = text\n",
    "\n",
    "        if self.cache is not None:\n",
    "            self.cache[text] = compressed\n",
    "        return compressed\n",
    "# -----------------------\n",
    "# Dataset\n",
    "# -----------------------\n",
    "# including KeyBERT tweaks, enable/disable compression; keybert kwargs are parameters for the compresser (model, top_n, etc)\n",
    "class ImageTextCSV(Dataset):\n",
    "    def __init__(self, csv_path: str, preprocess, tokenizer, text_ctx_len: int = None,\n",
    "                 use_keybert: bool = True, keybert_kwargs: Optional[Dict[str, Any]] = None):\n",
    "        self.items: List[tuple[str, str]] = []\n",
    "        self.preprocess = preprocess\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_ctx_len = text_ctx_len  # open_clip models typically 77 for CLIP; some support more\n",
    "\n",
    "        with open(csv_path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            assert \"image_path\" in reader.fieldnames and \"caption\" in reader.fieldnames, \\\n",
    "                \"CSV must have columns: image_path, caption\"\n",
    "            for row in reader:\n",
    "                self.items.append((row[\"image_path\"], row[\"caption\"]))\n",
    "        \n",
    "        # if enabled build a keyword compressor with provided kwargs\n",
    "        # stash on self so it can be called inside getitem\n",
    "        self.compressor = None\n",
    "        if use_keybert:\n",
    "            keybert_kwargs = keybert_kwargs or {}\n",
    "            self.compressor = KeywordCompressor(\n",
    "                tokenizer = self.tokenizer,\n",
    "                **keybert_kwargs\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img_path, caption = self.items[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.preprocess(image)  # tensor CHW\n",
    "        # open_clip tokenizer pads/truncates to the model's context length automatically\n",
    "        # text_tokens = self.tokenizer([caption])[0]\n",
    "        # return image, text_tokens\n",
    "\n",
    "        if self.compressor is not None:\n",
    "            text_for_clip = self.compressor(caption)\n",
    "        else:\n",
    "            text_for_clip = caption\n",
    "\n",
    "        text_tokens = self.tokenizer([text_for_clip])[0] \n",
    "        return image, text_tokens\n",
    "    \n",
    "        # runs caption through keybert to get packed summary before tokenization\n",
    "        # tokenizes compressed text, guaranteeing it fits within the token limit\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # batch: List[(image_tensor, text_ids)]\n",
    "    images, texts = zip(*batch)  # images: tuple of tensors, texts: tuple of 1D LongTensors\n",
    "    images = torch.stack(images, dim=0)\n",
    "    texts = torch.stack(texts, dim=0)\n",
    "    return images, texts\n",
    "\n",
    "# -----------------------\n",
    "# Build model + data\n",
    "# -----------------------\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(cfg.model_name, pretrained=cfg.pretrained)\n",
    "tokenizer = open_clip.get_tokenizer(cfg.model_name)\n",
    "\n",
    "# packs configs into a dict for passing to dataset\n",
    "keybert_args = dict(\n",
    "    model_name=cfg.keybert_model_name,\n",
    "    top_n=cfg.keybert_top_n,\n",
    "    ngram_range=cfg.keybert_ngram_range,\n",
    "    use_mmr=cfg.keybert_use_mmr,\n",
    "    diversity=cfg.keybert_diversity,\n",
    "    max_text_tokens=cfg.max_text_tokens,\n",
    "    enable_cache=cfg.cache_keywords,\n",
    ")\n",
    "\n",
    "# flip cfg.use_keybert to toggle keyword extraction\n",
    "# dataset with KeyBERT integration\n",
    "dataset = ImageTextCSV(cfg.train_csv, preprocess, tokenizer\n",
    "                       use_keybert=cfg.use_keybert,\n",
    "                       keybert_kwargs=keybert_args)\n",
    "\n",
    "\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=cfg.num_workers,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "device = torch.device(cfg.device)\n",
    "model = model.to(device)\n",
    "\n",
    "# -----------------------\n",
    "# Choose what to fine-tune\n",
    "# -----------------------\n",
    "def set_trainable(mode: str):\n",
    "    # Freeze everything first\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    if mode == \"full\":\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    elif mode == \"proj_only\":\n",
    "        # Train the projection heads + logit_scale for effective adaptation\n",
    "        # Projection layers typically named:\n",
    "        #   model.visual.proj (ViT projection)\n",
    "        #   model.text_projection (text projection)\n",
    "        # Also enable logit_scale (learnable temperature)\n",
    "        if hasattr(model, \"visual\") and hasattr(model.visual, \"proj\"):\n",
    "            for p in model.visual.proj.parameters() if hasattr(model.visual.proj, \"parameters\") else [model.visual.proj]:\n",
    "                p.requires_grad = True\n",
    "        if hasattr(model, \"text_projection\"):\n",
    "            if hasattr(model.text_projection, \"parameters\"):\n",
    "                for p in model.text_projection.parameters():\n",
    "                    p.requires_grad = True\n",
    "            else:\n",
    "                model.text_projection.requires_grad_(True)\n",
    "\n",
    "        model.logit_scale.requires_grad_(True)\n",
    "\n",
    "    elif mode == \"text_only\":\n",
    "        # Unfreeze text encoder + text_projection + logit_scale\n",
    "        if hasattr(model, \"transformer\"):\n",
    "            for p in model.transformer.parameters():\n",
    "                p.requires_grad = True\n",
    "        if hasattr(model, \"text_projection\"):\n",
    "            if hasattr(model.text_projection, \"parameters\"):\n",
    "                for p in model.text_projection.parameters():\n",
    "                    p.requires_grad = True\n",
    "            else:\n",
    "                model.text_projection.requires_grad_(True)\n",
    "        model.logit_scale.requires_grad_(True)\n",
    "\n",
    "    elif mode == \"vision_only\":\n",
    "        # Unfreeze vision encoder + visual.proj + logit_scale\n",
    "        if hasattr(model, \"visual\"):\n",
    "            for p in model.visual.parameters():\n",
    "                p.requires_grad = True\n",
    "        model.logit_scale.requires_grad_(True)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown finetune_mode: {mode}\")\n",
    "\n",
    "set_trainable(cfg.finetune_mode)\n",
    "\n",
    "# Verify\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable params: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")\n",
    "\n",
    "# -----------------------\n",
    "# Optimizer & Scheduler\n",
    "# -----------------------\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(params, lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(loader)*cfg.max_epochs)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=cfg.amp)\n",
    "\n",
    "# -----------------------\n",
    "# Training (CLIP loss)\n",
    "# -----------------------\n",
    "def clip_contrastive_loss(logits_per_image, logits_per_text):\n",
    "    batch_size = logits_per_image.size(0)\n",
    "    labels = torch.arange(batch_size, device=logits_per_image.device)\n",
    "    loss_i = torch.nn.functional.cross_entropy(logits_per_image, labels)\n",
    "    loss_t = torch.nn.functional.cross_entropy(logits_per_text, labels)\n",
    "    return (loss_i + loss_t) / 2\n",
    "\n",
    "best_loss = math.inf\n",
    "\n",
    "for epoch in range(cfg.max_epochs):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "\n",
    "    for step, (images, texts) in enumerate(loader):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        texts = texts.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=cfg.amp):\n",
    "            # open_clip forward returns logits in temperature-scaled space already\n",
    "            logits_per_image, logits_per_text = model(images, texts)\n",
    "            loss = clip_contrastive_loss(logits_per_image, logits_per_text)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        if cfg.grad_clip is not None:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(params, cfg.grad_clip)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        running += loss.item()\n",
    "        if (step + 1) % 50 == 0:\n",
    "            avg = running / 50\n",
    "            print(f\"Epoch {epoch+1}/{cfg.max_epochs} | Step {step+1}/{len(loader)} | Loss {avg:.4f}\")\n",
    "            running = 0.0\n",
    "\n",
    "    # Save checkpoint each epoch\n",
    "    ckpt_path = os.path.join(cfg.out_dir, f\"{cfg.model_name}-{cfg.pretrained}-epoch{epoch+1}.pt\")\n",
    "    torch.save({\"model\": model.state_dict(), \"config\": cfg.__dict__}, ckpt_path)\n",
    "    print(f\"Saved: {ckpt_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
