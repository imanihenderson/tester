{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feabb3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_clip.py\n",
    "import os\n",
    "import math\n",
    "import csv\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "import open_clip\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_name: str = \"ViT-B-32\"\n",
    "    pretrained: str = \"laion400m_e32\"   # or laion400m_e31\n",
    "    train_csv: str = \"data/train.csv\"\n",
    "    batch_size: int = 64\n",
    "    num_workers: int = 4\n",
    "    max_epochs: int = 5\n",
    "    lr: float = 5e-5\n",
    "    weight_decay: float = 0.02\n",
    "    grad_clip: float = 1.0\n",
    "    amp: bool = True\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    out_dir: str = \"checkpoints\"\n",
    "    finetune_mode: str = \"proj_only\"  # one of: \"full\", \"proj_only\", \"text_only\", \"vision_only\"\n",
    "    seed: int = 42\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "torch.manual_seed(cfg.seed)\n",
    "os.makedirs(cfg.out_dir, exist_ok=True)\n",
    "\n",
    "# -----------------------\n",
    "# Dataset\n",
    "# -----------------------\n",
    "class ImageTextCSV(Dataset):\n",
    "    def __init__(self, csv_path: str, preprocess, tokenizer, text_ctx_len: int = None):\n",
    "        self.items: List[tuple[str, str]] = []\n",
    "        self.preprocess = preprocess\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_ctx_len = text_ctx_len  # open_clip models typically 77 for CLIP; some support more\n",
    "\n",
    "        with open(csv_path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            assert \"image_path\" in reader.fieldnames and \"caption\" in reader.fieldnames, \\\n",
    "                \"CSV must have columns: image_path, caption\"\n",
    "            for row in reader:\n",
    "                self.items.append((row[\"image_path\"], row[\"caption\"]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img_path, caption = self.items[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.preprocess(image)  # tensor CHW\n",
    "        # open_clip tokenizer pads/truncates to the model's context length automatically\n",
    "        text_tokens = self.tokenizer([caption])[0]\n",
    "        return image, text_tokens\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # batch: List[(image_tensor, text_ids)]\n",
    "    images, texts = zip(*batch)  # images: tuple of tensors, texts: tuple of 1D LongTensors\n",
    "    images = torch.stack(images, dim=0)\n",
    "    texts = torch.stack(texts, dim=0)\n",
    "    return images, texts\n",
    "\n",
    "# -----------------------\n",
    "# Build model + data\n",
    "# -----------------------\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(cfg.model_name, pretrained=cfg.pretrained)\n",
    "tokenizer = open_clip.get_tokenizer(cfg.model_name)\n",
    "\n",
    "dataset = ImageTextCSV(cfg.train_csv, preprocess, tokenizer)\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=cfg.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=cfg.num_workers,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "device = torch.device(cfg.device)\n",
    "model = model.to(device)\n",
    "\n",
    "# -----------------------\n",
    "# Choose what to fine-tune\n",
    "# -----------------------\n",
    "def set_trainable(mode: str):\n",
    "    # Freeze everything first\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    if mode == \"full\":\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "    elif mode == \"proj_only\":\n",
    "        # Train the projection heads + logit_scale for effective adaptation\n",
    "        # Projection layers typically named:\n",
    "        #   model.visual.proj (ViT projection)\n",
    "        #   model.text_projection (text projection)\n",
    "        # Also enable logit_scale (learnable temperature)\n",
    "        if hasattr(model, \"visual\") and hasattr(model.visual, \"proj\"):\n",
    "            for p in model.visual.proj.parameters() if hasattr(model.visual.proj, \"parameters\") else [model.visual.proj]:\n",
    "                p.requires_grad = True\n",
    "        if hasattr(model, \"text_projection\"):\n",
    "            if hasattr(model.text_projection, \"parameters\"):\n",
    "                for p in model.text_projection.parameters():\n",
    "                    p.requires_grad = True\n",
    "            else:\n",
    "                model.text_projection.requires_grad_(True)\n",
    "\n",
    "        model.logit_scale.requires_grad_(True)\n",
    "\n",
    "    elif mode == \"text_only\":\n",
    "        # Unfreeze text encoder + text_projection + logit_scale\n",
    "        if hasattr(model, \"transformer\"):\n",
    "            for p in model.transformer.parameters():\n",
    "                p.requires_grad = True\n",
    "        if hasattr(model, \"text_projection\"):\n",
    "            if hasattr(model.text_projection, \"parameters\"):\n",
    "                for p in model.text_projection.parameters():\n",
    "                    p.requires_grad = True\n",
    "            else:\n",
    "                model.text_projection.requires_grad_(True)\n",
    "        model.logit_scale.requires_grad_(True)\n",
    "\n",
    "    elif mode == \"vision_only\":\n",
    "        # Unfreeze vision encoder + visual.proj + logit_scale\n",
    "        if hasattr(model, \"visual\"):\n",
    "            for p in model.visual.parameters():\n",
    "                p.requires_grad = True\n",
    "        model.logit_scale.requires_grad_(True)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown finetune_mode: {mode}\")\n",
    "\n",
    "set_trainable(cfg.finetune_mode)\n",
    "\n",
    "# Verify\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable params: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")\n",
    "\n",
    "# -----------------------\n",
    "# Optimizer & Scheduler\n",
    "# -----------------------\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(params, lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=len(loader)*cfg.max_epochs)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=cfg.amp)\n",
    "\n",
    "# -----------------------\n",
    "# Training (CLIP loss)\n",
    "# -----------------------\n",
    "def clip_contrastive_loss(logits_per_image, logits_per_text):\n",
    "    batch_size = logits_per_image.size(0)\n",
    "    labels = torch.arange(batch_size, device=logits_per_image.device)\n",
    "    loss_i = torch.nn.functional.cross_entropy(logits_per_image, labels)\n",
    "    loss_t = torch.nn.functional.cross_entropy(logits_per_text, labels)\n",
    "    return (loss_i + loss_t) / 2\n",
    "\n",
    "best_loss = math.inf\n",
    "\n",
    "for epoch in range(cfg.max_epochs):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "\n",
    "    for step, (images, texts) in enumerate(loader):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        texts = texts.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=cfg.amp):\n",
    "            # open_clip forward returns logits in temperature-scaled space already\n",
    "            logits_per_image, logits_per_text = model(images, texts)\n",
    "            loss = clip_contrastive_loss(logits_per_image, logits_per_text)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        if cfg.grad_clip is not None:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(params, cfg.grad_clip)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        running += loss.item()\n",
    "        if (step + 1) % 50 == 0:\n",
    "            avg = running / 50\n",
    "            print(f\"Epoch {epoch+1}/{cfg.max_epochs} | Step {step+1}/{len(loader)} | Loss {avg:.4f}\")\n",
    "            running = 0.0\n",
    "\n",
    "    # Save checkpoint each epoch\n",
    "    ckpt_path = os.path.join(cfg.out_dir, f\"{cfg.model_name}-{cfg.pretrained}-epoch{epoch+1}.pt\")\n",
    "    torch.save({\"model\": model.state_dict(), \"config\": cfg.__dict__}, ckpt_path)\n",
    "    print(f\"Saved: {ckpt_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
